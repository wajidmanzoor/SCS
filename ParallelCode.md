# Our code 

## Host Program

```cpp
1: Read Graph
2: Get Core values of each vertex
3: ku = min(core[QID], N2-1)
4: kl = 0
5: ubD = N2-1
6: Calculate distance from query vertex
7: Apply intial reduction rules 
8: Compress the tasks and status.
9: while True do
10:     Process Tasks 
11:     Expand Task
12:     if stopFlag then
13:         break
14:     end if
15: end while
```

## 1. Read Graphs
The Graph should be edge list (seperated by tab ) with the first line represents numVerticies and numEdges
    The Data is stored in 
        1. pstart : Neighbors offset
        2. edges : Neighbor list
        3. degree : Degree of each vertex 
        4. q_dist : Distance from query vertex of each vertex
        5. core : Core value of each vertex

## 2. Get Core values of each vertex (currently on CPU)
- Calculated the core values of each vertex and stores that in Core array. 
TODO: Move to GPU

## 3. Calculate distance from query vertex (currenlty on CPU)
- Calculates the distance of all vertices from query vertex and stores that info in q_dist

## 4. IntialReductionRules (GPU)
- Removes vertices that have core values less than or equal to the initial maximum minimum degree and a distance from the query vertex is greater than or equal to the upper bound size minus 1.

Suppose we have $n$ vertices, whose core values are stored in an array called *core* and distances from the query vertex are stored in an array called *dist_q*. We divide the vertices into partitions of size $ psize = \frac{n}{TOTAL_WARPS}+1$. This means the total number of partitions will be equal to *TOTAL_WARPS*.

We allocate memory for two arrays of size $pSize \times TOTAL_WARPS$. These arrays will store the vertices and their status if they satisfy certain conditions. Additionally, we allocate an array called *NumCounter* of size *TOTAL_WARPS* to keep track of the number of vertices written in each partition.

The processing unit in this context is a warp. The threads of each warp will read *psize* vertices and check if each vertex satisfies the given conditions. If a vertex satisfies the conditions, the thread will write the vertex and its status to its corresponding partitions in the output arrays. To avoid race conditions, threads inside a warp use atomic add operation to determine the next location where the vertex will be written. the status of an vertex will be set to zero except if it is a query vertex.

Once a warp has processed all *psize* vertices, the first thread of the warp will write the local counter (i.e., the total number of vertices written in the partition) to the *NumCounter* array at the location corresponding to the warp's ID. Additionally, the first thread of each warp will atomically add the local counter to a global counter to keep track of the total number of vertices written.

**Input**: Core Value Array, Distance from Query Vertex Array

```cpp
// Assumption: local_counter is in shared memory
1: if laneId == 0 then
2:     local_counter[warpId] ← 0
3: end if
4: __syncwarp()
5: start,writeOffset ← warpId * pSize
6: end ← min((warpId + 1) * pSize, size)
7: total ← end - start
8: for i ← laneId to total - 1 step 32 do
9:     vertex ← start + i
10:    if coreValues[vertex] > lowerBoundDegree and distanceFromQID[vertex] < (upperBoundSize - 1) then
11:        loc ← atomicAdd(local_counter[warpId], 1)
12:        taskList[loc + writeOffset] ← vertex
13:        if vertex == queryVertex then
14:            taskStatus[loc + writeOffset] ← 1
15:        else
16:            taskStatus[loc + writeOffset] ← 0
17:        end if
18:    end if
19: end for
20: __syncwarp()
21: if laneId == 0 then
22:    numEntries[warpId] ← local_counter[warpId]
23:    atomicAdd(globalCounter, local_counter[warpId])
24: end if
```

**Output**: Vertex Array, Status Array, Num Counter Array, Global Count

<img src="pics\Intial Reduction Rules.png"/>

## 5. CompressTask (GPU)
- Optimize the storage of vertex and status arrays returned by the InitialReductionRules Kernel.

The vertex and status arrays generated by the initial reduction rules contain segments where each partition begins with actual vertices and their statuses, followed by garbage values. To streamline memory usage, we aim to condense these arrays into a global counter-sized block of contiguous memory.

To achieve this, we allocate two arrays, *ReducedVertices* and *ReducedStatus*, both sized according to the global counter. Each processing unit operates independently. The first thread of each unit reads the total number of elements in its partition from the *NumCounter* array. Subsequently, each thread within the unit reads vertices from its partition and writes them into *ReducedVertices*, while their statuses are written into *ReducedStatus*.

```cpp
1: ui start = warpId * pSize
2: int temp = warpId - 1
3: ui total = numEntries[warpId]
4: ui writeOffset = 0 
5: for i ← laneId to total - 1 step 32 do
6:     while (temp >= 0) do
7:         writeOffset += numEntries[temp]
8:         temp--
9:     end while
10:     outputTasks[writeOffset + i] = taskList[start + i]
11:     outputStatus[writeOffset + i] = taskStatus[start + i]
12: end for
```

**Input**: Vertex Array, Status Array, Num Counter Array, Global Count
**Output**: Reduced Vertex Array, Reduces Status Array 

<img src="pics/Compress Task.png"/>

## 6. Process Task (GPU) 
- Calculates the minimum degree of each Task.

- Compares and updates the Maximum minimum degree.

- Calculates the Ustar for each Task

- Calculates the size of each Task

We allocate memory for three arrays: *Task*, *Status*, and *Offset*. Each array is of size $pSize \times TOTAL_warpS$ where *pSize* is user-defined. The *Task* array stores the vertices for all subgraphs that need processing. The *Status* array uses the following convention: 0 for vertices in C, 1 for vertices in R, and 2 for vertices in neither C nor R. The *Offset* array stores the starting index of each task within a partition.

Additionally, we allocate memory for three more arrays: Size, Ustar, and NumTasks, each of size $TOTAL_warpS$. These arrays store the size of each task, the ustar value for each task (-1 indicates no ustar found), and the total number of tasks in a partition, respectively.

The processing unit is a warp. Each warp processes tasks within a partition sequentially. The threads of a warp read the tasks of their corresponding partition and use `__shfl_down_sync` to compute the minimum degree, size, and connection score. The vertex with the maximum connection score is selected as the ustar.

Since a warp has only 32 threads and the size of a task can exceed 32, intermediate results for size, minimum degree, connection score, and ustar are stored in shared memory. Each time 32 vertices of a task are processed, the first thread of the warp updates the results in shared memory to obtain the final result.

Once all vertices of a task are processed, the first thread of the warp writes the results to the global memory arrays *Size* and *Ustar*. The first thread also updates the maximum minimum degree using atomicMax. Subsequently, the warp moves on to the next task in the partition.
 
```CPP
Assumption: sharedSize, sharedDegree, sharedUstar, sharedScore are in shared memory

1: if laneId = 0 then
2:     sharedSize[warpId], sharedScore[warpId], sharedUstar[warpId], sharedDegree[warpId] ← 0, 0, -1, UINT_MAX
3: syncthreads()

4: startIndex, endIndex ← warpId * pSize, (warpId + 1) * pSize - 1
5: totalTasks ← taskOffset[endIndex]

6: for iter ← 0 to totalTasks do
7:     start, end, total ← taskOffset[startIndex + iter], taskOffset[startIndex + iter + 1], taskOffset[startIndex + iter + 1] - taskOffset[startIndex + iter]

8:     for i ← laneId to total step 32 do
9:         index, vertex, status, startNeighbor, endNeighbor ← startIndex + start + i, taskList[startIndex + start + i], taskStatus[startIndex + start + i], neighborOffset[taskList[startIndex + start + i]], neighborOffset[taskList[startIndex + start + i] + 1]


10:         score, currentMinDegree, ustar, temp3 ← 0, 0, -1, UINT_MAX

11:         if status = 0 then
12:             for j ← startNeighbor to endNeighbor do
13:                 resultIndex ← findIndexKernel(taskList, startIndex + start, startIndex + end, neighbors[j])
14:                 if resultIndex ≠ -1 and taskStatus[resultIndex] = 1 then
15:                     score ← score + (1 / degree[neighbors[j]])
16:         if score > 0 then
17:             score ← score + (degree[vertex] / dmax)

18:         for offset ← WARPSIZE / 2 to 0 step /2 do
19:             temp, otherId ← __shfl_down_sync(0xFFFFFFFF, score, offset), __shfl_down_sync(0xFFFFFFFF, index, offset)
20:             if temp > score and temp > 0 then
21:                 score, index ← temp, otherId
22:         ustar, score ← __shfl_sync(0xFFFFFFFF, index, 0), __shfl_sync(0xFFFFFFFF, score, 0)

23:         if status = 1 then
24:             for j ← startNeighbor to endNeighbor do
25:                 resultIndex ← findIndexKernel(taskList, startIndex + start, startIndex + end, neighbors[j])
26:                 if resultIndex ≠ -1 and taskStatus[resultIndex] = 1 then
27:                     currentMinDegree ← currentMinDegree + 1

28:         for offset ← WARPSIZE / 2 to 0 step /2 do
29:             temp2 ← __shfl_down_sync(0xFFFFFFFF, currentMinDegree, offset)
30:             if temp2 > 0 and temp2 < temp3 then
31:                 currentMinDegree, temp3 ← temp2, temp2
32:         currentMinDegree ← __shfl_sync(0xFFFFFFFF, temp3, 0)

33:         for offset ← WARPSIZE / 2 to 0 step /2 do
34:             status ← status + __shfl_down_sync(0xFFFFFFFF, status, offset)
35:         current_size ← __shfl_sync(0xFFFFFFFF, status, 0)

36:         if i % 32 = 0 then
37:             sharedSize[warpId], sharedDegree[warpId], sharedScore[warpId], sharedUstar[warpId] ← sharedSize[warpId] + current_size, min(sharedDegree[warpId], currentMinDegree), max(sharedScore[warpId], score), (ustar if score > sharedScore[warpId] else sharedUstar[warpId])

38: if laneId = 0 then
39:     if lowerBoundSize ≤ sharedSize[warpId] ≤ upperBoundSize and sharedDegree[warpId] ≠ UINT_MAX then
40:         atomicMax(lowerBoundDegree, sharedDegree[warpId])
41:     writeOffset ← warpId * pSize
42:     if sharedScore[warpId] > 0 then
43:         ustarList[writeOffset + iter] ← sharedUstar[warpId]
44:     subgraphSize[writeOffset + iter] ← sharedSize[warpId]
45:     sharedSize[warpId], sharedScore[warpId], sharedUstar[warpId], sharedDegree[warpId] ← 0, 0, -1, UINT_MAX

```

**Output**: Task Array, Status Array, offset array, Num Tasks, Size , Ustar

<img src="pics/ustar and size.png"/>


## 7. Expand Task (GPU)
- This kernel applies a reduction rule to prune the set $\( R \)$.
- Using the current task (subgraph) and Ustar, it generates new tasks (subgraphs) if the potential maximum degree exceeds $\( k_{\text{lower}} \)$. 
- These newtasks are then added to the task array.

**Input**: Task Array, Status Array, Offset, NumTasks, Size, Ustar

The processing unit is a warp, with threads within the warp sequentially reading tasks from their corresponding partitions. To create new tasks, each thread first checks the vertex. If the vertex fails any reduction rule, its status is updated to 2. If the vertex is identified as a ustar, its status is updated to 1. This modification creates a new task, denoted as C + ustar and R - ustar, replacing the old task. The thread then writes the vertex to the next partition if it does not fail any reduction rules, using the task offset of that partition to determine the write offset and employing atomic addition to secure the next write location. This process ensures that new tasks, C and R - ustar, are generated in the subsequent partition.

After new task is written in the subsequent partition, the first thread of the warp will update the *Offset* and *NumTask* value of that partition.

```cpp
Assumption: sharedCounter is in shared memory
1: if laneId = 0 then sharedCounter[warpId] ← 0
2: __syncwarp()
3: startIndex, endIndex, totalTasks ← warpId * pSize, (warpId + 1) * pSize - 1, taskOffset[(warpId + 1) * pSize - 1]

4: for iter ← 0 to totalTasks - 1 do
5:     start, end, total ← taskOffset[startIndex + iter], taskOffset[startIndex + iter + 1], taskOffset[startIndex + iter + 1] - taskOffset[startIndex + iter]

6:     for i ← laneId to total - 1 step 32 do
7:         ind ← startIndex + start + i
8:         vertex, status ← taskList[ind], taskStatus[ind]

9:         if subgraphSize[warpId * pSize + iter] < upperBoundSize and ustarList[warpId * pSize + iter] ≠ -1 then
10:             bufferNum ← warpId + 2
11:             if bufferNum > TOTAL_WARPS then bufferNum ← 1
12:             totalTasksWrite ← taskOffset[bufferNum * pSize - 1]
13:             writeOffset ← (bufferNum - 1) * pSize + taskOffset[(bufferNum - 1) * pSize + totalTasksWrite]

14:             if vertex ≠ taskList[ustarList[warpId * pSize + iter]] and status ≠ 2 then
15:                 loc ← atomicAdd(sharedCounter[warpId], 1)
16:                 taskList[writeOffset + loc], taskStatus[writeOffset + loc] ← vertex, status
17:             end if
18:         end if
19:     end for

20:     if laneId = 0 then
21:         if subgraphSize[warpId * pSize + iter] < upperBoundSize and ustarList[warpId * pSize + iter] ≠ -1 then
22:             flag, bufferNum ← 0, warpId + 2
23:             if bufferNum > TOTAL_WARPS then bufferNum ← 1
24:             totalTasksWrite ← taskOffset[bufferNum * pSize - 1]
25:             taskStatus[taskList[ustarList[warpId * pSize + iter]]] ← 1
26:             taskOffset[(bufferNum - 1) * pSize + totalTasksWrite + 1], taskOffset[bufferNum * pSize - 1] ← taskOffset[(bufferNum - 1) * pSize + totalTasksWrite] + sharedCounter[warpId], taskOffset[bufferNum * pSize - 1] + 1
27:         end if

28:         sharedCounter[warpId] ← 0
29:     end if
30: end for
```

**Output** : Task Array, Status Array, Offset, NumTasks

<img src="pics/Expand Task.png"/>
 